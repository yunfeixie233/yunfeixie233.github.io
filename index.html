<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <title>Yunfei Xie</title>
  <meta name="author" content="Yunfei Xie">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/hust_icon.png" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:60%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Yunfei Xie | 谢云飞
                  </p>
                  <p>
                    I'm a first-year CS PhD student in the Department of Computer Science at
                    <a href="https://www.rice.edu/">Rice University</a>, advised by
                    <a href="https://weichen582.github.io/">Prof.&nbsp;Chen Wei</a>.
                    My current research interests lie in LLM agents and reinforcement learning.
                  </p>
                  <p>
                    Specifically, I am working on LLM post-training data and algorithms for agents. My aim is to develop context learning and reinforcement learning frameworks that enable LLMs to continuously self-improve based on environmental observations, learn from experience, and strengthen their reasoning and agentic capabilities.
                  </p>
                  <p>
                    I completed my bachelor's degree in the
                    School of Artificial Intelligence and Automation
                    at <a href="https://english.hust.edu.cn/">Huazhong University of Science &amp; Technology</a>.
                    During my undergraduate studies, I was honored to work as a research intern with Prof. <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a> and Prof. <a href="https://cihangxie.github.io/">Cihang Xie</a> in <a href="https://ucsc-vlaa.github.io/">VLAA</a> at <a href="https://www.ucsc.edu/">UC, Santa Cruz</a>, and with Prof. <a href="https://www.cs.jhu.edu/~ayuille/">Alan Yuille</a> and Dr.<a href="https://meijieru.com/">Jieru Mei</a> in <a href="https://ccvl.jhu.edu/">CCVL</a> at <a href="https://www.jhu.edu/">Johns Hopkins University</a>.
                  </p>
                  <p style="font-weight:bold;">
                    I am open for collaborations in research. Also, I am looking for potential intern positions in the summer of 2026.
                  </p>               
                  <p style="text-align:center">
                    <a href="mailto:xieyunfei01@gmail.com">Email</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?user=rD8RC-EAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                    <a href="https://github.com/yunfeixie233">Github</a> &nbsp;/&nbsp;
                    <a href="https://x.com/xiynfi1520580">Twitter</a> &nbsp;/&nbsp;
                    <a href="data/CV_yunfeixie.pdf" target="_blank">Resume</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/me.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%; height: 270px;" alt="profile photo" src="images/me.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <h2>Selected Publications</h2>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <!-- Play to Generalize Paper -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center;">
                    <div class="two" id='play_to_generalize_image'></div>
                    <img src='images/vigal.png' width=100%>
                  </div>
                </td>
                <td style="width:75%;vertical-align:middle">
                  <a href="https://yunfeixie233.github.io/ViGaL/" target="_blank">
                    <span class="papertitle">Play to Generalize: Learning to Reason Through Game Play</span>
                  </a>
                  <br>
                  <strong>Yunfei Xie</strong>,
                  <a href="https://openreview.net/profile?id=~Yinsong_Ma1" target="_blank">Yinsong Ma</a>,
                  <a href="https://voidrank.github.io/" target="_blank">Shiyi Lan</a>,
                  <a href="https://www.cs.jhu.edu/~ayuille/" target="_blank">Alan Yuille</a>,
                  <a href="https://lambert-x.github.io/" target="_blank">Junfei Xiao</a><sup>†</sup>,
                  <a href="https://weichen582.github.io/" target="_blank">Chen Wei</a><sup>§</sup>
                  <br>
                  <em>ICLR</em>, 2026
                  <br>
                  <a href="https://arxiv.org/abs/2506.08011" target="_blank">paper</a> / <a href="https://yunfeixie233.github.io/ViGaL/" target="_blank">website</a>
                  <br>
                  <strong>TL;DR:</strong>
                  We show that post-training multimodal language models with reinforcement learning on simple arcade games such as Snake improves performance on math and broader reasoning benchmarks, without any exposure to worked solutions, equations, or diagrams during training.
                </td>
              </tr>

              <!-- MEMO Paper -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center;">
                    <div class="two" id='memo_image'></div>
                    <img src='images/memo.png' width=100%>
                  </div>
                </td>
                <td style="width:75%;vertical-align:middle">
                  <a href="https://yunfeixie233.github.io/MEMO/" target="_blank">
                    <span class="papertitle">MEMO: Memory-Augmented Model Context Optimization for Robust Multi-Turn Multi-Agent LLM Games</span>
                  </a>
                  <br>
                  <strong>Yunfei Xie</strong>,
                  <a href="https://www.kevin-ai.com/" target="_blank">Kevin Wang</a>,
                  <a href="https://x.com/bobbycxy" target="_blank">Bobby Cheng</a>,
                  <a href="https://scholar.google.com/citations?user=L5XDYTIAAAAJ&hl=en" target="_blank">Jianzhu Yao</a>,
                  <a href="https://jamessand.github.io/" target="_blank">Zhizhou Sha</a>,
                  <a href="https://www.linkedin.com/in/alex-d/" target="_blank">Alexander Duffy</a>,
                  <a href="https://scholar.google.com/citations?user=34s2YS0AAAAJ&hl=en" target="_blank">Yihan Xi</a>,
                  <a href="https://www.hongyuanmei.com/" target="_blank">Hongyuan Mei</a>,
                  <a href="https://scholar.google.com/citations?user=Up0UYEYAAAAJ&hl=en" target="_blank">Cheston Tan</a>,
                  <a href="https://weichen582.github.io/" target="_blank">Chen Wei</a>,
                  <a href="https://scholar.google.com/citations?user=lPycXNcAAAAJ&hl=en" target="_blank">Pramod Viswanath</a>,
                  <a href="https://scholar.google.com/citations?user=pxFyKAIAAAAJ&hl=en" target="_blank">Zhangyang Wang</a>
                  <br>
                  <em>arxiv</em>, 2025
                  <br>
                  <a href="https://yunfeixie233.github.io/MEMO/static/MEMO.pdf" target="_blank">paper</a> / <a href="https://yunfeixie233.github.io/MEMO/" target="_blank">website</a>
                  <br>
                  <strong>TL;DR:</strong>
                  We propose MEMO, a self-play framework that optimizes inference-time context through memory retention and exploration, raising mean win rate from 24.9% to 49.5% for GPT-4o-mini across five text-based games while reducing run-to-run variance.
                </td>
              </tr>

              <!-- MedTrinity-25M Paper -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center;">
                    <div class="two" id='nuvo_image'></div>
                    <img src='images/e209a4d74e8a7183bc6b5987c789aa6.png' width=100%>
                  </div>
                </td>
                <td style="width:75%;vertical-align:middle">
                  <a href="https://yunfeixie233.github.io/MedTrinity-25M/" target="_blank">
                    <span class="papertitle">MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine</span>
                  </a>
                  <br>
                  <strong>Yunfei Xie</strong>,
                  <a href="https://scholar.google.com/citations?user=A2rMGkQAAAAJ&hl=en">Ce Zhou</a>,
                  <a href="https://github.com/HeartyHaven">Lang Gao</a>,
                  <a href="https://scholar.google.com/citations?user=RSn2gykAAAAJ&hl=en">Juncheng Wu</a>,
                  <a href="https://xhl-video.github.io/xianhangli/">Xianhang Li</a>,
                  <a href="https://zhouhy.org/">Hong-Yu Zhou</a>,
                  <a href="https://shengliu66.github.io/">Liu Sheng</a>,
                  <a href="https://profiles.stanford.edu/lei-xing">Lei Xing</a>,
                  <a href="https://www.james-zou.com/">James Zou</a>,
                  <a href="https://cihangxie.github.io/">Cihang Xie</a>,
                  <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>
                  <br>
                  <em>ICLR</em>, 2025
                  <br>
                  <a href="https://www.arxiv.org/abs/2408.02900" target="_blank">paper</a> / <a href="https://yunfeixie233.github.io/MedTrinity-25M/" target="_blank">website</a>
                  <br>
                  <strong>TL;DR:</strong>
                  We develop a RAG-enhanced automated data synthesis pipeline that generates 25 million high-quality multimodal medical samples with multigranular annotations for vision-language model pretraining.
                </td>
              </tr>

              <!-- Story-Adapter Paper -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center;">
                    <div class="two" id='story_adapter_image'></div>
                    <img src='images/story_adapter.png' width=100%>
                  </div>
                </td>
                <td style="width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2410.06244" target="_blank">
                    <span class="papertitle">Story-Adapter: A Training-free Iterative Framework For Long Story Visualization</span>
                  </a>
                  <br>
                  <a href="https://github.com/jwmao1" target="_blank">Jiawei Mao</a><sup>*</sup>,
                  <a href="https://xk-huang.github.io/" target="_blank">Xiaoke Huang</a><sup>*</sup>,
                  <strong>Yunfei Xie</strong>,
                  <a href="" target="_blank">Yuanqi Chang</a>,
                  <a href="https://thefllood.github.io/mudehui.github.io/" target="_blank">Mude Hui</a>,
                  <a href="https://scholar.google.com/citations?user=JHTNigYAAAAJ&hl=en" target="_blank">Bingjie Xu</a>,
                  <a href="https://yuyinzhou.github.io/" target="_blank">Yuyin Zhou</a>
                  <br>
                  <em>ICLR</em>, 2026
                  <br>
                  <a href="https://arxiv.org/abs/2410.06244" target="_blank">paper</a> / 
                  <a href="https://github.com/jwmao1/story-adapter" target="_blank">code</a>
                  <br>
                  <strong>TL;DR:</strong>
                  We proposed a training-free and efficient framework for generating long stories with up to 100 frames. We designed an iterative paradigm that progressively refines the process by integrating text and global constraints, achieving more precise interactions and improved semantic consistency throughout the story.
                </td>
              </tr>

              <!-- From Pixels to Objects Paper -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center;">
                    <div class="two" id='xie2024lgformer_2'></div>
                    <img src='images/xie2024lgformer.png' width=100%>
                  </div>
                </td>
                <td style="width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2409.01353" target="_blank">
                    <span class="papertitle">From Pixels to Objects: A Hierarchical Approach for Part and Object Segmentation Using Local and Global Aggregation</span>
                  </a>
                  <br>
                  <strong>Yunfei Xie</strong>,
                  <a href="https://cihangxie.github.io/">Cihang Xie</a>,
                  <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                  <a href="https://meijieru.com/">Jieru Mei</a>
                  <br>
                  <em>ECCV</em>, 2024
                  <br>
                  <a href="https://arxiv.org/abs/2409.01353" target="_blank">paper</a>
                  <br>
                  <strong>TL;DR:</strong>
                  We developed a hierarchical superpixel-based model that simultaneously addresses two conflicting needs in segmentation: local detail for parts and global context for objects. By employing local aggregation for superpixels in part segmentation and global aggregation for group tokens in object segmentation, we achieved state-of-the-art performance in part and object segmentation.
                </td>
              </tr>

              <!-- o1 Medical Paper -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center;">
                    <div class="two" id='o1_medical_image'></div>
                    <img src='images/o1_medical.png' width=80%>
                  </div>
                </td>
                <td style="width:75%;vertical-align:middle">
                  <a href="http://arxiv.org/abs/2409.15277" target="_blank">
                    <span class="papertitle">A Preliminary Study of o1 in Medicine: Are We Closer to an AI Doctor?</span>
                  </a>
                  <br>
                  <strong>Yunfei Xie</strong>,
                  <a href="https://chtholly17.github.io/">Juncheng Wu</a>,
                  <a href="https://www.haqtu.me/">Haoqin Tu</a>,
                  <a href="https://laos-y.github.io/#about">Siwei Yang</a>,
                  <a href="https://bzhao.me/">Bingchen Zhao</a>,
                  <a href="https://ys-zong.github.io/">Yongshuo Zong</a>,
                  <a href="https://andy-jqa.github.io/">Qiao Jin</a>,
                  <a href="https://cihangxie.github.io/">Cihang Xie</a>,
                  <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>
                  <br>
                  <em>arxiv</em>, 2024
                  <br>
                  <a href="http://arxiv.org/abs/2409.15277" target="_blank">paper</a> / 
                  <a href="https://github.com/UCSC-VLAA/o1_medical" target="_blank">code</a> / 
                  <a href="https://huggingface.co/datasets/UCSC-VLAA/o1_medical" target="_blank">data</a>
                  <br>
                  <strong>TL;DR:</strong>
                  We introduce a comprehensive medical benchmark to evaluate OpenAI o1's clinical capabilities and report preliminary results indicating stronger clinical reasoning, together with known limitations and actionable directions for improvement.
                </td>
              </tr>

              <!-- Few-shot Medical Image Segmentation Paper -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center;">
                    <div class="two" id='xie2024lgformer_1'></div>
                    <img src='images/svformer.png' width=100%>
                  </div>
                </td>
                <td style="width:75%;vertical-align:middle">
                  <a href="https://yunfeixie233.github.io/" target="_blank">
                    <span class="papertitle">Few-shot Medical Image Segmentation via Supervoxel Transformer</span>
                  </a>
                  <br>
                  <strong>Yunfei Xie</strong>,
                  <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,                
                  <a href="https://cihangxie.github.io/">Cihang Xie</a>,
                  <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>,
                  <a href="https://meijieru.com/">Jieru Mei</a>
                  <br>
                  <em>arxiv</em>, 2024
                  <br>
                  <a href="paper/svformer.pdf" target="_blank">paper</a>
                  <br>
                  <strong>TL;DR:</strong>
                  To address the complexity of 3D medical volume representations, we proposed supervoxels, which are more flexible and semantically meaningful, suitable for 3D organ structures. 
                </td>   
              </tr>

              <!-- Brain Tumor Segmentation Paper -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center;">
                    <div class="two" id='brats_image'></div>
                    <img src='images/d745a5dcffa390db0c85b20669aa1b1.png' width=100%>
                  </div>
                </td>
                <td style="width:75%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/10635335" target="_blank">
                    <span class="papertitle">Brain Tumor Segmentation Through SuperVoxel Transformer</span>
                  </a>
                  <br>
                  <strong>Yunfei Xie</strong>,
                  <a href="https://github.com/Skylight-Lark">Ce Zhou</a>,
                  <a href="https://meijieru.com/">Jieru Mei</a>,
                  <a href="https://xhl-video.github.io/xianhangli/">Xianhang Li</a>,
                  <a href="https://cihangxie.github.io/">Cihang Xie</a>,
                  <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>
                  <br>
                  <em>ISBI</em>, 2024
                  <br>
                  <a href="https://ieeexplore.ieee.org/abstract/document/10635335" target="_blank">paper</a>
                  <br>
                  <strong>TL;DR:</strong>
                  We developed two CNN-Transformer hybrid models as part of the BraTS-ISBI 2024 challenge to create brain tumor segmentation models with broad applicability. 
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <h2>Education</h2>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle;text-align: right;">
                  <img src="images/hust_icon.png" style="width: 65%; height: 99px;">
                </td>
                <td width="75%" valign="center">
                  Undergrad in Artificial Intelligence, 
                  <a href="http://english.hust.edu.cn/" target="_blank">Huazhong University of Science & Technology</a>
                  <br>
                  2021.09 - Present, Wuhan, China     
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <h2>Experience</h2>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle; text-align: right;">
                  <img src="images/ucsc_icon.png" style="width: 65%; height: 99px;">
                </td>
                <td width="75%" valign="center">
                  <p>
                    <strong>Dec. 2023 - Feb. 2025: Research Intern, 
                    <a href="https://ucsc-vlaa.github.io/index.html" target="_blank" rel="external">VLAA</a></strong>,  
                    <a href="https://www.ucsc.edu/" target="_blank" rel="external">University of California, Santa Cruz</a><br>
                    Supervisor: Prof. <a href="https://yuyinzhou.github.io/" target="_blank" rel="external">Yuyin Zhou</a> and
                    Prof. <a href="https://cihangxie.github.io/" target="_blank" rel="external">Cihang Xie</a> and Dr. 
                    <a href="https://meijieru.com/" target="_blank" rel="external">Jieru Mei</a><br>                     
                    Focus: Multimodal Language Models for Medicine<br>
                  </p>   
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle;text-align: right;">
                  <img src="images/jhu_icon.png" style="width: 65%; height: 99px;">
                </td>
                <td width="75%" valign="center">
                  <p>
                    <strong>Jul. 2023 - Dec. 2023: Research Intern, 
                    <a href="https://ccvl.jhu.edu/" target="_blank" rel="external">CCVL</a></strong>, 
                    <a href="https://www.jhu.edu/" target="_blank" rel="external">Johns Hopkins University</a><br>
                    Supervisor: Prof. <a href="https://cihangxie.github.io/" target="_blank" rel="external">Cihang Xie</a> and Dr. 
                    <a href="https://meijieru.com/" target="_blank" rel="external">Jieru Mei</a><br>
                    Focus: Multi-Level Segmentation, Few-Shot Medical Segmentation<br>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <div style="width: 100%; background-color: #ffffff; padding: 20px 0;">
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
              <tbody>
                <tr>
                  <td>
                    <h2 style="margin-bottom: 5px;">Reviewer</h2>
                  </td>
                </tr>
              </tbody>
            </table>       
            <table width="100%" align="center" border="0" cellpadding="10">
              <tbody>
                <tr>
                  <td width="75%" valign="center">
                    <p style="margin-top: 0;">
                      <strong>2025: ICCV, ICLR, CVPR, ICML, TMM, NeurIPS, TPAMI</strong><br>
                      <strong>2024: CVPR, ICML, IEEE ISBI</strong>                      
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>
            <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=BCBdlSjHUY88t5iDbJGITrKix8hSMJb_2ieeiEkjs4E&cl=ffffff&w=a"></script>
          </div>
          
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    This website template was borrowed from <a href="https://jonbarron.info/">  Jon Barron </a>. <br>
                    Last updated on July 29th, 2024.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
    </tbody>
  </table>

  </div>
</body>

</html>
