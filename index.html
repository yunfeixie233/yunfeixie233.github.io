<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <title>Yunfei Xie</title>
  <meta name="author" content="Yunfei Xie">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/hust_icon.png" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:60%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Yunfei Xie | 谢云飞
                  </p>
                  <p>
                    I'm a first-year Ph.D. student in the Department of Computer Science at
                    <a href="https://www.rice.edu/">Rice University</a>, under the supervision of
                    <a href="https://weichen582.github.io/">Prof.&nbsp;Chen Wei</a>.
                  </p>
                  <p>
                    I completed my bachelor's degree in the
                    School of Artificial Intelligence and Automation
                    at <a href="https://english.hust.edu.cn/">Huazhong University of Science &amp; Technology</a>.
                    During my undergraduate studies, I was honored to work as a research intern with Prof. <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a> and Prof. <a href="https://cihangxie.github.io/">Cihang Xie</a> in <a href="https://ucsc-vlaa.github.io/">VLAA</a> at <a href="https://www.ucsc.edu/">UC, Santa Cruz</a>, and with Prof. <a href="https://www.cs.jhu.edu/~ayuille/">Alan Yuille</a> and Dr.<a href="https://meijieru.com/">Jieru Mei</a> in <a href="https://ccvl.jhu.edu/">CCVL</a> at <a href="https://www.jhu.edu/">Johns Hopkins University</a>.
                  </p>
                  <p>
                    My research interests mainly include multimodal language models and reinforcement learning.
                  </p>
                  <p style="font-weight:bold;">
                    I am open for collaborations in research. Also, I am looking for potential intern positions in the summer of 2026.
                  </p>               
                  <p style="text-align:center">
                    <a href="mailto:xieyunfei01@gmail.com">Email</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?user=rD8RC-EAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                    <a href="https://github.com/yunfeixie233">Github</a> &nbsp;/&nbsp;
                    <a href="https://x.com/xiynfi1520580">Twitter</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/me.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%; height: 270px;" alt="profile photo" src="images/me.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <h2>Publications</h2>
          <div class="tab">
            <strong class="tabItem" style="color: #e76f51;">[Multimodal]</strong><span style="color: #000; font-size: 20px;">,</span>
            <strong class="tabItem" style="color: #219ebc;">[Segmentation]</strong><span style="color: #000; font-size: 20px;">,</span>
            <strong class="tabItem" style="color: #f4a261;">[Generation]</strong>
          </div>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <!-- Play to Generalize Paper -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center;">
                    <div class="two" id='play_to_generalize_image'></div>
                    <img src='images/vigal.png' width=100%>
                  </div>
                </td>
                <td style="width:75%;vertical-align:middle">
                  <h2 style="color: #e76f51;">
                    <strong class="tabItem" style="color: #e76f51;">[Multimodal]</strong>
                  </h2>  
                  <a href="https://yunfeixie233.github.io/ViGaL/" target="_blank">
                    <span class="papertitle">Play to Generalize: Learning to Reason Through Game Play</span>
                  </a>
                  <br>
                  <strong>Yunfei Xie</strong>,
                  <a href="https://openreview.net/profile?id=~Yinsong_Ma1" target="_blank">Yinsong Ma</a>,
                  <a href="https://voidrank.github.io/" target="_blank">Shiyi Lan</a>,
                  <a href="https://www.cs.jhu.edu/~ayuille/" target="_blank">Alan Yuille</a>,
                  <a href="https://lambert-x.github.io/" target="_blank">Junfei Xiao</a><sup>†</sup>,
                  <a href="https://weichen582.github.io/" target="_blank">Chen Wei</a><sup>§</sup>
                  <br>
                  <em>arxiv</em>, 2025
                  <br>
                  <a href="https://arxiv.org/abs/2506.08011" target="_blank">paper</a> / <a href="https://yunfeixie233.github.io/ViGaL/" target="_blank">website</a>
                  <br>
                  <strong>TL;DR:</strong>
                  We propose a novel post-training paradigm, Visual Game Learning, or ViGaL, where MLLMs develop out-of-domain generalization of multimodal reasoning through playing arcade-like games. Specifically, we show that post-training a 7B-parameter MLLM via RLVR on simple arcade-like games, e.g. Snake, significantly enhances its downstream performance on multimodal math benchmarks like MathVista, and on multi-discipline questions like MMMU, without seeing any worked solutions, equations, or diagrams during RL, suggesting the capture of transferable reasoning skills.
                </td>
              </tr>

              <!-- MedTrinity-25M Paper -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center;">
                    <div class="two" id='nuvo_image'></div>
                    <img src='images/e209a4d74e8a7183bc6b5987c789aa6.png' width=100%>
                  </div>
                </td>
                <td style="width:75%;vertical-align:middle">
                  <h2 style="color: #e76f51;">
                    <strong class="tabItem" style="color: #e76f51;">[Multimodal]</strong>
                  </h2>  
                  <a href="https://yunfeixie233.github.io/MedTrinity-25M/" target="_blank">
                    <span class="papertitle">MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine</span>
                  </a>
                  <br>
                  <strong>Yunfei Xie</strong>,
                  <a href="https://scholar.google.com/citations?user=A2rMGkQAAAAJ&hl=en">Ce Zhou</a>,
                  <a href="https://github.com/HeartyHaven">Lang Gao</a>,
                  <a href="https://scholar.google.com/citations?user=RSn2gykAAAAJ&hl=en">Juncheng Wu</a>,
                  <a href="https://xhl-video.github.io/xianhangli/">Xianhang Li</a>,
                  <a href="https://zhouhy.org/">Hong-Yu Zhou</a>,
                  <a href="https://shengliu66.github.io/">Liu Sheng</a>,
                  <a href="https://profiles.stanford.edu/lei-xing">Lei Xing</a>,
                  <a href="https://www.james-zou.com/">James Zou</a>,
                  <a href="https://cihangxie.github.io/">Cihang Xie</a>,
                  <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>
                  <br>
                  <em>ICLR</em>, 2025
                  <br>
                  <a href="https://www.arxiv.org/abs/2408.02900" target="_blank">paper</a> / <a href="https://yunfeixie233.github.io/MedTrinity-25M/" target="_blank">website</a>
                  <br>
                  <strong>TL;DR:</strong>
                  To scale up multimodal medical datasets, we developed an automated pipeline to generate multigranular visual and textual annotations for any given medical image. Based on this pipeline, we introduced MedTrinity-25M, a comprehensive, large-scale dataset containing over 25 million images, including detailed ROIs.
                </td>
              </tr>
              
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center;">
                    <div class="two" id='xie2024lgformer_2'></div>
                    <img src='images/xie2024lgformer.png' width=100%>
                  </div>
                </td>
                
                <td style="width:75%;vertical-align:middle">
                  <h2 style="color: #219ebc;">
                    <strong class="tabItem" style="color: #219ebc;">[Segmentation]</strong>
                  </h2>  
                  <a href="https://arxiv.org/abs/2409.01353" target="_blank">
                    <span class="papertitle">From Pixels to Objects: A Hierarchical Approach for Part and Object Segmentation Using Local and Global Aggregation</span>
                  </a>
                  <br>
                  <strong>Yunfei Xie</strong>,
                  <a href="https://cihangxie.github.io/">Cihang Xie</a>,
                  <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                  <a href="https://meijieru.com/">Jieru Mei</a>
                  <br>
                  <em>ECCV</em>, 2024
                  <br>
                  <a href="https://arxiv.org/abs/2409.01353" target="_blank">paper</a>
                  <br>
                  <strong>TL;DR:</strong>
                  We developed a hierarchical superpixel-based model that simultaneously addresses two conflicting needs in segmentation: local detail for parts and global context for objects. By employing local aggregation for superpixels in part segmentation and global aggregation for group tokens in object segmentation, we achieved state-of-the-art performance in part and object segmentation.
                 </td>
              </tr>

              <!-- Existing Papers -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center;">
                    <div class="two" id='o1_medical_image'></div>
                    <img src='images/o1_medical.png' width=80%>
                  </div>
                </td>
                <td style="width:75%;vertical-align:middle">
                  <h2 style="color: #e76f51;">
                    <strong class="tabItem" style="color: #e76f51;">[Multimodal]</strong>
                  </h2>  
                  <a href="http://arxiv.org/abs/2409.15277" target="_blank">
                    <span class="papertitle">A Preliminary Study of o1 in Medicine: Are We Closer to an AI Doctor?</span>
                  </a>
                  <br>
                  <strong>Yunfei Xie</strong>,
                  <a href="https://chtholly17.github.io/">Juncheng Wu</a>,
                  <a href="https://www.haqtu.me/">Haoqin Tu</a>,
                  <a href="https://laos-y.github.io/#about">Siwei Yang</a>,
                  <a href="https://bzhao.me/">Bingchen Zhao</a>,
                  <a href="https://ys-zong.github.io/">Yongshuo Zong</a>,
                  <a href="https://andy-jqa.github.io/">Qiao Jin</a>,
                  <a href="https://cihangxie.github.io/">Cihang Xie</a>,
                  <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>
                  <br>
                  <em>arxiv</em>, 2024
                  <br>
                  <a href="http://arxiv.org/abs/2409.15277" target="_blank">paper</a> / 
                  <a href="https://github.com/UCSC-VLAA/o1_medical" target="_blank">code</a> / 
                  <a href="https://huggingface.co/datasets/UCSC-VLAA/o1_medical" target="_blank">data</a>
                  <br>
                  <strong>TL;DR:</strong>
                  We benchmarked OpenAI's o1-preview model on a comprehensive medical dataset covering 37 diverse tasks, including understanding, reasoning, multilingual capabilities, and agent interaction. Additionally, we presented preliminary findings that enhance clinical reasoning in o1 and discussed the model's limitations and future directions for improvement.
                </td>
                </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center;">
                    <div class="two" id='story_adapter_image'></div>
                    <img src='images/story_adapter.png' width=100%>
                  </div>
                </td>
                <td style="width:75%;vertical-align:middle">
                  <h2 style="color: #f4a261;">
                    <strong class="tabItem" style="color: #f4a261;">[Generation]</strong>
                  </h2>  
                  <a href="https://arxiv.org/abs/2410.06244" target="_blank">
                    <span class="papertitle">Story-Adapter: A Training-free Iterative Framework For Long Story Visualization</span>
                  </a>
                  <br>
                  <a href="https://github.com/jwmao1" target="_blank">Jiawei Mao</a><sup>*</sup>,
                  <a href="https://xk-huang.github.io/" target="_blank">Xiaoke Huang</a><sup>*</sup>,
                  <strong>Yunfei Xie</strong>,
                  <a href="" target="_blank">Yuanqi Chang</a>,
                  <a href="https://thefllood.github.io/mudehui.github.io/" target="_blank">Mude Hui</a>,
                  <a href="https://scholar.google.com/citations?user=JHTNigYAAAAJ&hl=en" target="_blank">Bingjie Xu</a>,
                  <a href="https://yuyinzhou.github.io/" target="_blank">Yuyin Zhou</a>
                  <br>
                  <em>arxiv</em>, 2024
                  <br>
                  <a href="https://arxiv.org/abs/2410.06244" target="_blank">paper</a> / 
                  <a href="https://github.com/jwmao1/story-adapter" target="_blank">code</a>
                  <br>
                    <strong>
                      TL;DR: </strong>
                    We proposed a training-free and efficient framework for generating long stories with up to 100 frames. We designed an iterative paradigm that progressively refines the process by integrating text and global constraints, achieving more precise interactions and improved semantic consistency throughout the story.                  </strong>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center;">
                    <div class="two" id='xie2024lgformer_1'></div>
                    <img src='images/svformer.png' width=100%>
                  </div>
                </td>
                <td style="width:75%;vertical-align:middle">
                  <h2 style="color: #219ebc;">
                    <strong class="tabItem" style="color: #219ebc;">[Segmentation]</strong>
                  </h2>   
                  <a href="https://yunfeixie233.github.io/" target="_blank">
                    <span class="papertitle">Few-shot Medical Image Segmentation via Supervoxel Transformer</span>
                  </a>
                  <br>
                  <strong>Yunfei Xie</strong>,
                  <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,                
                  <a href="https://cihangxie.github.io/">Cihang Xie</a>,
                  <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>,
                  <a href="https://meijieru.com/">Jieru Mei</a>
                  <br>
                  <em>arxiv</em>, 2024
                  <br>
                  <a href="paper/svformer.pdf" target="_blank">paper</a>
                  <br>
                  <strong>
                    TL;DR: </strong>
                    To address the complexity of 3D medical volume representations, we proposed supervoxels, which are more flexible and semantically meaningful, suitable for 3D organ structures. 
                </td>   
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center;">
                    <div class="two" id='brats_image'></div>
                    <img src='images/d745a5dcffa390db0c85b20669aa1b1.png' width=100%>
                  </div>
                </td>
                <td style="width:75%;vertical-align:middle">
                  <h2 style="color: #219ebc;">
                    <strong class="tabItem" style="color: #219ebc;">[Segmentation]</strong>
                  </h2> 
                  <a href="https://ieeexplore.ieee.org/abstract/document/10635335" target="_blank">
                    <span class="papertitle">Brain Tumor Segmentation Through SuperVoxel Transformer</span>
                  </a>
                  <br>
                  <strong>Yunfei Xie</strong>,
                  <a href="https://github.com/Skylight-Lark">Ce Zhou</a>,
                  <a href="https://meijieru.com/">Jieru Mei</a>,
                  <a href="https://xhl-video.github.io/xianhangli/">Xianhang Li</a>,
                  <a href="https://cihangxie.github.io/">Cihang Xie</a>,
                  <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>
                  <br>
                  <em>ISBI</em>, 2024
                  <br>
                  <a href="https://ieeexplore.ieee.org/abstract/document/10635335" target="_blank">paper</a>
                  <br>
                  <strong>
                    TL;DR: </strong>
                    We developed two CNN-Transformer hybrid models as part of the BraTS-ISBI 2024 challenge to create brain tumor segmentation models with broad applicability. 
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <h2>Education</h2>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle;text-align: right;">
                  <img src="images/hust_icon.png" style="width: 65%; height: 99px;">
                </td>
                <td width="75%" valign="center">
                  Undergrad in Artificial Intelligence, 
                  <a href="http://english.hust.edu.cn/" target="_blank">Huazhong University of Science & Technology</a>
                  <br>
                  2021.09 - Present, Wuhan, China     
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <h2>Experience</h2>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle; text-align: right;">
                  <img src="images/ucsc_icon.png" style="width: 65%; height: 99px;">
                </td>
                <td width="75%" valign="center">
                  <p>
                    <strong>Dec. 2023 - Feb. 2025: Research Intern, 
                    <a href="https://ucsc-vlaa.github.io/index.html" target="_blank" rel="external">VLAA</a></strong>,  
                    <a href="https://www.ucsc.edu/" target="_blank" rel="external">University of California, Santa Cruz</a><br>
                    Supervisor: Prof. <a href="https://yuyinzhou.github.io/" target="_blank" rel="external">Yuyin Zhou</a> and
                    Prof. <a href="https://cihangxie.github.io/" target="_blank" rel="external">Cihang Xie</a> and Dr. 
                    <a href="https://meijieru.com/" target="_blank" rel="external">Jieru Mei</a><br>                     
                    Focus: Multimodal Language Models for Medicine<br>
                  </p>   
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle;text-align: right;">
                  <img src="images/jhu_icon.png" style="width: 65%; height: 99px;">
                </td>
                <td width="75%" valign="center">
                  <p>
                    <strong>Jul. 2023 - Dec. 2023: Research Intern, 
                    <a href="https://ccvl.jhu.edu/" target="_blank" rel="external">CCVL</a></strong>, 
                    <a href="https://www.jhu.edu/" target="_blank" rel="external">Johns Hopkins University</a><br>
                    Supervisor: Prof. <a href="https://cihangxie.github.io/" target="_blank" rel="external">Cihang Xie</a> and Dr. 
                    <a href="https://meijieru.com/" target="_blank" rel="external">Jieru Mei</a><br>
                    Focus: Multi-Level Segmentation, Few-Shot Medical Segmentation<br>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <div style="width: 100%; background-color: #ffffff; padding: 20px 0;">
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
              <tbody>
                <tr>
                  <td>
                    <h2 style="margin-bottom: 5px;">Reviewer</h2>
                  </td>
                </tr>
              </tbody>
            </table>       
            <table width="100%" align="center" border="0" cellpadding="10">
              <tbody>
                <tr>
                  <td width="75%" valign="center">
                    <p style="margin-top: 0;">
                      <strong>2025: ICCV, ICLR, CVPR, ICML, TMM, NeurIPS, TPAMI</strong><br>
                      <strong>2024: CVPR, ICML, IEEE ISBI</strong>                      
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>
            <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=BCBdlSjHUY88t5iDbJGITrKix8hSMJb_2ieeiEkjs4E&cl=ffffff&w=a"></script>
          </div>
          
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    This website template was borrowed from <a href="https://jonbarron.info/">  Jon Barron </a>. <br>
                    Last updated on July 29th, 2024.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
    </tbody>
  </table>

  </div>
</body>

</html>
